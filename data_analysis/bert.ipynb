{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from transformers import AutoTokenizer \n",
    "import os \n",
    "import random \n",
    "import torch.nn as nn \n",
    "from transformers import AutoConfig \n",
    "from transformers import AutoModel  \n",
    "from transformers import get_cosine_schedule_with_warmup \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from transformers import AdamW \n",
    "import wandb \n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "wandb.login()\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "scaler = torch.cuda.amp.GradScaler() \n",
    "\n",
    "SEED = 0 \n",
    "N_FOLDS = 5 \n",
    "INPUT_DIR = '../data_collection/data' \n",
    "MAX_LEN = 320 \n",
    "\n",
    "MODEL_NAME = 'bert-base-uncased' \n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME) \n",
    "LR = 2e-5 \n",
    "WEIGHT_DECAY = 1e-6 \n",
    "N_EPOCHS = 1\n",
    "WARM_UP_RATIO = 0.1 \n",
    "\n",
    "BS = 32\n",
    "ACCUMULATE = 100\n",
    "MIXED_PRECISION = False \n",
    "\n",
    "\n",
    "# def create_folds(data):\n",
    "#     data[\"kfold\"] = -1\n",
    "#     data = data.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "#     num_bins = int(np.floor(1 + np.log2(len(data))))\n",
    "    \n",
    "#     data.loc[:, \"bins\"] = pd.cut(\n",
    "#         data[\"likes_count\"], bins=num_bins, labels=False\n",
    "#     )\n",
    "#     kf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "#     for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
    "#         data.loc[v_, 'kfold'] = f\n",
    "#     data = data.drop(\"bins\", axis=1)\n",
    "    \n",
    "#     return data\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "  random.seed(set_seed) \n",
    "  os.environ[\"PYTHONHASHED\"] = str(seed) \n",
    "  np.random.seed(seed) \n",
    "  torch.manual_seed(seed) \n",
    "  torch.cuda.manual_seed(seed) \n",
    "  torch.cuda.manual_seed_all(seed) \n",
    "  torch.backends.cudnn.deterministic = True \n",
    "  torch.backends.cudnn.benchmark = False \n",
    "\n",
    "class QiitaDataset(Dataset):\n",
    "\n",
    "  def __init__(self, df):\n",
    "    self.texts = df['body'].tolist() \n",
    "    self.labels = df['likes_count']\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts) \n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    text =self.texts[item] \n",
    "    label = self.labels[item] \n",
    "\n",
    "    tok = TOKENIZER.encode_plus(\n",
    "        text, \n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True, \n",
    "        padding='max_length',\n",
    "        return_attention_mask=True, \n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "\n",
    "    d = {\n",
    "        \"input_ids\": torch.tensor(tok['input_ids'], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(tok[\"attention_mask\"], dtype=torch.long),\n",
    "         \"token_type_ids\": torch.tensor(tok[\"token_type_ids\"], dtype=torch.long),\n",
    "         \"label\": torch.tensor(label, dtype=torch.double),\n",
    "    }\n",
    "\n",
    "    return d \n",
    "\n",
    "class QiitaModel(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(QiitaModel, self).__init__() \n",
    "    self.config = AutoConfig.from_pretrained(MODEL_NAME) \n",
    "    self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    self.regressor = nn.Linear(self.config.hidden_size, 1) \n",
    "\n",
    "  def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "    outputs = self.bert(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids\n",
    "    )\n",
    "    sequence_output = outputs['last_hidden_state'][:, 0] \n",
    "    logits = self.regressor(sequence_output)\n",
    "\n",
    "    return logits \n",
    "\n",
    "  def loss_fn(self, logits, label):\n",
    "    loss = nn.L1Loss(reduction='mean')(logits[:, 0], label)\n",
    "    return loss\n",
    "\n",
    "def validation_loop(valid_loader, model):\n",
    "  model.eval() \n",
    "  preds = [] \n",
    "  for d in tqdm(valid_loader): \n",
    "    with torch.no_grad(): \n",
    "      logits = model(\n",
    "          d['input_ids'].to(device),\n",
    "          d['attention_mask'].to(device),\n",
    "          d['token_type_ids'].to(device)\n",
    "      )\n",
    "    preds.append(logits[:, 0]) \n",
    "  y_pred = torch.hstack(preds).cpu().numpy() \n",
    "  y_true = valid_loader.dataset.labels \n",
    "  mae_loss = mean_absolute_error(y_true, y_pred) \n",
    "  return mae_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame() \n",
    "for i in range(1, 13):\n",
    "  df_tmp = pd.read_csv(f\"../data_collection/data/2020-{i:02}.csv\", encoding='utf8')\n",
    "  df = pd.concat([df, df_tmp], ignore_index=True) \n",
    "\n",
    "df = df[['likes_count', 'body']] \n",
    "df = df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = create_folds(df) \n",
    "train_df, valid_df = train_test_split(df, test_size=0.3)\n",
    "\n",
    "train_df = train_df.reset_index()\n",
    "valid_df = valid_df.reset_index()\n",
    "\n",
    "# train_index = train_df.query('kfold!=0').index.tolist() \n",
    "# valid_index = train_df.query('kfold==0').index.tolist() \n",
    "\n",
    "# set dataset \n",
    "train_dataset = QiitaDataset(train_df) \n",
    "valid_dataset = QiitaDataset(valid_df) \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS,\n",
    "                          pin_memory=True, shuffle=True, drop_last=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, \n",
    "                          pin_memory=True, shuffle=False, drop_last=False, num_workers=0)\n",
    "\n",
    "\n",
    "# set models \n",
    "model = QiitaModel() \n",
    "model.to(device) \n",
    "\n",
    "# set optimizer \n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY) \n",
    "max_train_steps = N_EPOCHS * len(train_loader) \n",
    "warmup_steps = int(max_train_steps * WARM_UP_RATIO) \n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=max_train_steps\n",
    ")\n",
    "\n",
    "wandb.init(project='Qiita_BERT') \n",
    "wandb.watch(model) \n",
    "\n",
    "\n",
    "set_seed() \n",
    "optimizer.zero_grad() \n",
    "train_iter_loss, valid_best_loss, all_step = 0, 999, 0 \n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "  print('epoch: ', epoch)\n",
    "  for d in tqdm(train_loader): \n",
    "    all_step += 1 \n",
    "    model.train() \n",
    "    if MIXED_PRECISION:\n",
    "      with torch.cuda.amp.autocast(): \n",
    "        logits = model(\n",
    "            d[\"input_ids\"].to(device),\n",
    "            d[\"attention_mask\"].to(device),\n",
    "            d[\"token_type_ids\"].to(device)\n",
    "        )\n",
    "        loss = model.loss_fn(logits, d['label'].float().to(device))\n",
    "        loss = loss / ACCUMULATE \n",
    "    else:\n",
    "      logits = model(\n",
    "          d['input_ids'].to(device), \n",
    "          d['attention_mask'].to(device), \n",
    "          d['token_type_ids'].to(device)\n",
    "      )\n",
    "      loss = model.loss_fn(logits, d['label'].float().to(device)) \n",
    "      loss = loss / ACCUMULATE \n",
    "\n",
    "    train_iter_loss += loss.item() \n",
    "\n",
    "    if MIXED_PRECISION:\n",
    "      scaler.scale(loss).backward() \n",
    "    else:\n",
    "      loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "    if all_step % ACCUMULATE == 0:\n",
    "        if MIXED_PRECISION:\n",
    "          scaler.step(optimizer) \n",
    "          scaler.update() \n",
    "        else:\n",
    "          optimizer.step() \n",
    "        optimizer.zero_grad() \n",
    "        scheduler.step() \n",
    "\n",
    "        valid_loss = validation_loop(valid_loader, model) \n",
    "        if valid_best_loss > valid_loss: \n",
    "          valid_best_loss = valid_loss \n",
    "          torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_iter_loss,\n",
    "            \"valid_loss\": valid_loss, \n",
    "            \"valid_best_loss\": valid_best_loss,\n",
    "        })\n",
    "        \n",
    "        train_iter_loss = 0 \n",
    "  \n",
    "wandb.finish()\n",
    "\n",
    "print(valid_best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiita",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7213027f19972ebf0eabaa3af4f9f37ae828f17e95ec243f0527716e3e78786"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
